{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HgjZsT2wHYV"
      },
      "source": [
        "<h1 align=\"center\">ðŸš€ RocketEval ðŸš€</h1>\n",
        "<h3 align=\"center\">Efficiently Evaluate LLMs using Google Colab</h3>\n",
        "\n",
        "This notebook will run a full evaluation process of [RocketEval](https://github.com/Joinn99/RocketEval-ICLR) using a single Tesla T4 GPU in Google Colab.\n",
        "\n",
        "RocketEval is an efficient automated evaluation framework for Large Language Models (LLMs) that uses a checklist-based grading approach. This notebook demonstrates how to:\n",
        "\n",
        "1. Set up the evaluation environment on Google Colab\n",
        "2. Run evaluations using a lightweight model (*Qwen2.5-0.5B-Instruct*) as the judge\n",
        "3. Evaluate multiple LLM responses on the *MT-Bench* dataset\n",
        "4. Generate evaluation scores and rankings efficiently with limited compute resources\n",
        "\n",
        "The entire process is optimized to run on a single Tesla T4 GPU, making it accessible for users with basic GPU resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNeMm1LHwSzG"
      },
      "source": [
        "------\n",
        "\n",
        "First, clone our github repo. This repository contains the implementation of RocketEval, an efficient automated evaluation framework for Large Language Models (LLMs) that uses a checklist-based grading approach. The repository includes:\n",
        "\n",
        "- Evaluation scripts and utilities\n",
        "- Example benchmark datasets (MT-Bench, AlpacaEval, etc.)\n",
        "- Model configurations for both API and local deployment\n",
        "- Documentation and examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFbdcafI-mfV"
      },
      "outputs": [],
      "source": [
        "# Clone the repo\n",
        "!git clone https://github.com/Joinn99/RocketEval-ICLR.git\n",
        "# Download the data\n",
        "!git clone https://huggingface.co/datasets/Joinn/RocketEval && mv RocketEval RocketEval-ICLR/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2OrirviwX3z"
      },
      "source": [
        "------\n",
        "\n",
        "Go to the target dir, and install the necessary dependencies.\n",
        "\n",
        "We need to:\n",
        "1. Change to the RocketEval directory\n",
        "2. Install the required packages from requirements.txt\n",
        "\n",
        "The requirements.txt file contains all the necessary Python packages including:\n",
        "- `vllm` for local model deployment\n",
        "- `openai` for API access\n",
        "- `scikit-learn` for evaluation metrics\n",
        "- Other utility packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsOsvxRO_LQE"
      },
      "outputs": [],
      "source": [
        "%cd RocketEval-ICLR\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLp8TjLGw4R5"
      },
      "source": [
        "------\n",
        "\n",
        "Next, we'll configure the evaluation parameters. This notebook demonstrates a minimal evaluation setup with the following key arguments:\n",
        "\n",
        "- `dataset`: [MT-Bench](https://huggingface.co/spaces/lmsys/mt-bench/tree/main), a compact LLM evaluation dataset\n",
        "- `generator`: [GPT-4](https://chatgpt.com/) for checklist generation (we'll use pre-generated checklists)\n",
        "- `judge`: [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct), chosen for efficient evaluation on a Tesla T4 GPU\n",
        "- `train_test`: Enables split evaluation using model sets from:\n",
        "  - Training: `config/rankings/mt-bench_train.json`\n",
        "  - Testing: `config/rankings/mt-bench_test.json`\n",
        "- `mode`: \"offline\" to utilize the local vLLM framework\n",
        "- `gpu_ids`: \"0\" for single GPU execution\n",
        "- `offline_config`: vLLM engine configuration file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWvKcVhVB6yz"
      },
      "outputs": [],
      "source": [
        "# Arguments for the evaluation task\n",
        "default_args=\"\"\"--dataset mt-bench --generator gpt-4o --judge Qwen2.5-0.5B-Instruct --train_test --mode offline --gpu_ids 0 --offline_config config/offline/colab.yaml\"\"\".split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oobdufUBX_qj"
      },
      "source": [
        "------\n",
        "\n",
        "Next, we start to run our task.\n",
        "\n",
        "We'll use the default arguments defined above to:\n",
        "1. Load the MT-Bench dataset\n",
        "2. Initialize the Qwen2.5-0.5B-Instruct judge model using vLLM\n",
        "3. Run the evaluation pipeline including:\n",
        "   - Checklist-based grading\n",
        "   - Score aggregation\n",
        "   - Model ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tsf_rEhCtkBH"
      },
      "outputs": [],
      "source": [
        "#@title Running Evaluation Task\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "rocketeval_dir = os.path.join(os.path.abspath(os.curdir), \"src\")\n",
        "sys.path.insert(0, rocketeval_dir)\n",
        "\n",
        "import time\n",
        "import openai\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "from rich.logging import RichHandler\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "from rocketeval.data.data_loader import load_target_models\n",
        "from rocketeval.task import checklist_task, judgment_task, ranking_task, score_task\n",
        "\n",
        "logging.getLogger('RootLogger').setLevel(logging.INFO)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(message)s\",\n",
        "    datefmt=\"[%X]\",\n",
        "    handlers=[RichHandler()],\n",
        "    force=True\n",
        ")\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"RocketEval Task Runner\")\n",
        "\n",
        "# Data\n",
        "parser.add_argument(\"--data_dir\", default=\"data/\", help=\"Data directory\")\n",
        "parser.add_argument(\"--config_dir\", default=\"config/\", help=\"Config directory\")\n",
        "\n",
        "# Model\n",
        "parser.add_argument(\"--dataset\", default=\"mt-bench\", help=\"Dataset name\")\n",
        "parser.add_argument(\"--generator\", default=\"gpt-4o\", help=\"Generator model\")\n",
        "parser.add_argument(\"--judge\", default=\"gpt-4o\", help=\"Judge model\")\n",
        "parser.add_argument(\"--labeler\", default=\"gpt-4o\", help=\"Labeler judge that provides labels\")\n",
        "parser.add_argument(\"--train_test\", action=\"store_true\", help=\"Use specific train-test split\")\n",
        "parser.add_argument(\"--gen_checklist\", action=\"store_true\", help=\"Generate checklist\")\n",
        "\n",
        "# Running Mode\n",
        "parser.add_argument(\"--mode\", choices=[\"api\", \"offline\"], help=\"Running mode, set to 'api' to use OpenAI API, set to 'offline' to use local models through vLLM\")\n",
        "parser.add_argument(\"--instant_api\", action=\"store_true\", help=\"Run using instant API.\")\n",
        "parser.add_argument(\"--api_parallel_size\", default=1, help=\"Number of parallel API calls, adjust based on your API rate limit.\")\n",
        "parser.add_argument(\"--offline_config\", default=\"config/offline/default.yaml\", help=\"Path to vLLM config file\")\n",
        "\n",
        "# Others\n",
        "parser.add_argument(\"--resume_from_task_id\", default=None, help=\"Task ID\")\n",
        "parser.add_argument(\"--keep_batch_files\", action=\"store_true\", help=\"Keep batch files\")\n",
        "parser.add_argument(\"--gpu_ids\", default=\"0\", help=\"GPU IDs, split by comma\")\n",
        "\n",
        "args = parser.parse_args(default_args)\n",
        "kwargs = vars(args)\n",
        "\n",
        "task_id = f\"{args.dataset}_{int(time.time())}\" \\\n",
        "    if args.resume_from_task_id is None \\\n",
        "    else args.resume_from_task_id\n",
        "\n",
        "if args.mode == \"api\":\n",
        "    client = openai.OpenAI()\n",
        "else:\n",
        "    client = None\n",
        "    if not os.path.exists(os.path.join(args.data_dir, \"batch\")):\n",
        "      os.makedirs(os.path.join(args.data_dir, \"batch\"))\n",
        "\n",
        "task_id = f\"{args.dataset}_{args.judge}_{int(time.time())}\" \\\n",
        "    if args.resume_from_task_id is None \\\n",
        "    else args.resume_from_task_id\n",
        "\n",
        "train_model_names = load_target_models(\n",
        "    data_dir=args.data_dir,\n",
        "    config_dir=args.config_dir,\n",
        "    dataset_name=args.dataset,\n",
        "    split=\"train\" if args.train_test else \"full\"\n",
        ")\n",
        "\n",
        "test_model_names = load_target_models(\n",
        "    data_dir=args.data_dir,\n",
        "    config_dir=args.config_dir,\n",
        "    dataset_name=args.dataset,\n",
        "    split=\"test\" if args.train_test else \"full\"\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(\"rich\")\n",
        "\n",
        "start_message = f\"\"\"[underline bold red on white blink]RocketEval[/]\n",
        "[bold yellow on red blink] Task Information[/]\n",
        "- Dataset: \"{args.dataset}\"\n",
        "- Judge: \"{args.judge}\"\n",
        "- Labeler: \"{args.labeler}\"\n",
        "- Task ID: \"{task_id}\"\n",
        "\"\"\".replace(\"\\t\", \"\")\n",
        "\n",
        "logger.info(start_message, extra={\"markup\": True})\n",
        "\n",
        "\n",
        "logger.info(f\"[bold yellow on red blink]RocketEval Completed[/]\", extra={\"markup\": True})\n",
        "\n",
        "if args.gen_checklist:\n",
        "    # I - Checklist Creation\n",
        "    logger.info(\n",
        "        \"[bold yellow on red blink]I. Checklist Creation[/]\", extra={\"markup\": True}\n",
        "    )\n",
        "\n",
        "    checklist_task(\n",
        "        client=client,\n",
        "        task_id=task_id,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    logger.info(\n",
        "        f\"[yellow]Checklist Creation completed.[/]\\n\\n\",\n",
        "        extra={\"markup\": True}\n",
        "    )\n",
        "else:\n",
        "    logger.info(\n",
        "        f\"[bold yellow on red blink]Checklist Creation skipped.[/]\", extra={\"markup\": True},\n",
        "    )\n",
        "\n",
        "# II - Judgment Creation\n",
        "logger.info(\n",
        "    \"[bold yellow on red blink]II. Judgment Creation[/]\", extra={\"markup\": True}\n",
        ")\n",
        "\n",
        "judgment_task(\n",
        "    model_names=train_model_names + test_model_names,\n",
        "    client=client,\n",
        "    task_id=task_id,\n",
        "    **kwargs\n",
        ")\n",
        "\n",
        "logger.info(\n",
        "    f\"[yellow]Judgment Creation completed.[/]\\n\\n\",\n",
        "    extra={\"markup\": True}\n",
        ")\n",
        "\n",
        "\n",
        "# III - Score Creation\n",
        "logger.info(\n",
        "    f\"[bold yellow on red blink]III. Score Creation[/]\",\n",
        "    extra={\"markup\": True}\n",
        ")\n",
        "\n",
        "score_task(\n",
        "    train_model_names=train_model_names,\n",
        "    test_model_names=test_model_names,\n",
        "    task_id=task_id,\n",
        "    **kwargs\n",
        ")\n",
        "\n",
        "logger.info(\n",
        "    f\"[yellow]Score Creation completed.[/]\\n\\n\",\n",
        "    extra={\"markup\": True}\n",
        ")\n",
        "\n",
        "\n",
        "# IV - Ranking\n",
        "logger.info(\n",
        "    f\"[bold yellow on red blink]IV. Ranking[/]\",\n",
        "    extra={\"markup\": True}\n",
        ")\n",
        "\n",
        "ranking = ranking_task(\n",
        "    model_names=test_model_names,\n",
        "    **kwargs\n",
        ")\n",
        "\n",
        "Console().print(Markdown(ranking.to_markdown()), justify=\"center\")\n",
        "\n",
        "logger.info(\n",
        "    f\"[yellow]Ranking completed.[/]\\n\\n\",\n",
        "    extra={\"markup\": True}\n",
        ")\n",
        "\n",
        "\n",
        "# Finish\n",
        "logger.info(\n",
        "    f\"[bold yellow on red blink]RocketEval Completed[/]\",\n",
        "    extra={\"markup\": True}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etZHRUBbq9IT"
      },
      "source": [
        "------\n",
        "\n",
        "After deriving the scores and rankings, you can also export the data to the [LMSYS Chatbot Arena](https://lmarena.ai/) format for further analysis using the official [notebook](https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTIVdMrqq9IT"
      },
      "outputs": [],
      "source": [
        "from rocketeval.tools.export import chatbot_arena_match\n",
        "from rocketeval.data.data_loader import load_target_models\n",
        "\n",
        "load_target_models(dataset_name=\"mt-bench\", split=\"test\")\n",
        "result = chatbot_arena_match(dataset_name=\"mt-bench\", judge=\"Qwen2.5-0.5B-Instruct\", model_names=test_model_names)\n",
        "result.to_json(\"matches.jsonl\", orient=\"records\", lines=True)\n",
        "\n",
        "result.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOYXJGx7zAqW"
      },
      "source": [
        "You can try on more powerful judge models and increase the number of test models to get a better and comprehensive evaluation result.\n",
        "\n",
        "------\n",
        "\n",
        "If you find this work useful in your research, please consider citing the following paper:\n",
        "```bibtex\n",
        "@inproceedings{wei2025rocketeval,\n",
        "    title={RocketEval: Efficient automated {LLM} evaluation via grading checklist},\n",
        "    author={Tianjun Wei and Wei Wen and Ruizhi Qiao and Xing Sun and Jianghong Ma},\n",
        "    booktitle={The Thirteenth International Conference on Learning Representations},\n",
        "    year={2025},\n",
        "    url={https://openreview.net/forum?id=zJjzNj6QUe}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}